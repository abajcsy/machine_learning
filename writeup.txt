Warming up to Classifiers
1. WU1: why is this computation equivalent to computing classification accuracy?

The classification accuracy 
	= 1/N * \sum_{k=1}^{N} [ datasets.TennisData.Y(k) == h.predictAll(datasets.TennisData.X)(k) ]
	= mean ( datasets.TennisData.Y == h.predictAll(datasets.TennisData.X) )
	
datasets.TennisData.Y > 0 converts all of the negative labels to 0 and keeps the positive labels
at 1. Call this new array a. Similarly, h.predictAll(datasets.TennisData.X) > 0) converts all of
the predicted negative labels to 0 and keeps the predicted positive labels at 1. Denote this array
as b.

a == b is exactly the same array as datasets.TennisData.Y == h.predictAll(datasets.TennisData.X),
so the computations are equivalent.


Decision Trees
1. WU2: We should see training accuracy (roughly) going down and test accuracy (roughly) going up. Why does training accuracy tend to go down? Why is test accuracy not monotonically increasing? You should also see jaggedness in the test curve toward the left. Why?

2. WU3: You should see training accuracy monotonically increasing and test accuracy making something like a hill. Which of these is guaranteed to happen a which is just something we might expect to happen? Why?


Nearest Neighbors
1. WU4: For the digits data, generate train/test curves for varying values of K and epsilon (you figure out what are good ranges, this time). Include those curves: do you see evidence of overfitting and underfitting? Next, using K=5, generate learning curves for this data.
2. WU5: A. First, get a histogram of the raw digits data in 784 dimensions. You'll probably want to use the exampleDistancefunction from KNN together with the plotting in HighD. B. Extend exampleDistance so that it can subsample features down to some fixed dimensionality. For example, you might write subsampleExampleDistance(x1,x2,D), where D is the target dimensionality. In this function, you should pick D dimensions at random (I would suggest generating a permutation of the number [1..784] and then taking the first D of them), and then compute the distance but only looking at those dimensions. C.Generate an equivalent plot to HighD with D in [2, 8, 32, 128, 512] but for the digits data rather than the random data. Include a copy of both plots and describe the differences.

Perceptron
1. WU6: Using the tools provided, generate (a) a learning curve (x-axis=number of training examples) for the perceptron (5 epochs) on the sentiment data and (b) a plot of number of epochs versus train/test accuracy on the entire dataset.

