Warming up to Classifiers
1. WU1: why is this computation equivalent to computing classification accuracy?

The classification accuracy 
	= 1/N * \sum_{k=1}^{N} [ datasets.TennisData.Y(k) == h.predictAll(datasets.TennisData.X)(k) ]
	= mean ( datasets.TennisData.Y == h.predictAll(datasets.TennisData.X) )
	
datasets.TennisData.Y > 0 converts all of the negative labels to 0 and keeps the positive labels
at 1. Call this new array a. Similarly, h.predictAll(datasets.TennisData.X) > 0) converts all of
the predicted negative labels to 0 and keeps the predicted positive labels at 1. Denote this array
as b.

a == b is exactly the same array as datasets.TennisData.Y == h.predictAll(datasets.TennisData.X),
so the computations are equivalent.


Decision Trees
1. WU2: We should see training accuracy (roughly) going down and test accuracy (roughly) going up. 
Why does training accuracy tend to go down? 
--> As you increase the number of input data points while keeping the same tree height, there is a higher likelihood of misclassifying new data points.
Why is test accuracy not monotonically increasing? 
--> The test accuracy is not monotonically increasing because the initially small samples of data are not sufficient to allow inference to the real data distribution. At a certain point during testing, the number of datapoints allows for a sufficiently accurate representation of the real data. 
You should also see jaggedness in the test curve toward the left. Why?
--> The ability of a small data sample to sufficiently represent the distribution is more unreliable than a large sample. This is represented by the initial jaggedness in the test curve during the first few small samples. 

2. WU3: You should see training accuracy monotonically increasing and test accuracy making something like a hill. Which of these is guaranteed to happen and which is just something we might expect to happen? Why?
--> We are guaranteed to see training accuracy monotonically increasing as the tree gets deeper because we are considering more features to partition the data. On the other hand, we expect that test accuracy will increase and then start to decrease in a hill-like fashion due to overfitting.  


Nearest Neighbors
1. WU4: For the digits data, generate train/test curves for varying values of K and epsilon (you figure out what are good ranges, this time). Include those curves: do you see evidence of overfitting and underfitting? Next, using K=5, generate learning curves for this data.
2. WU5: A. First, get a histogram of the raw digits data in 784 dimensions. You'll probably want to use the exampleDistancefunction from KNN together with the plotting in HighD. B. Extend exampleDistance so that it can subsample features down to some fixed dimensionality. For example, you might write subsampleExampleDistance(x1,x2,D), where D is the target dimensionality. In this function, you should pick D dimensions at random (I would suggest generating a permutation of the number [1..784] and then taking the first D of them), and then compute the distance but only looking at those dimensions. C.Generate an equivalent plot to HighD with D in [2, 8, 32, 128, 512] but for the digits data rather than the random data. Include a copy of both plots and describe the differences.

Perceptron
1. WU6: Using the tools provided, generate (a) a learning curve (x-axis=number of training examples) for the perceptron (5 epochs) on the sentiment data and (b) a plot of number of epochs versus train/test accuracy on the entire dataset.

